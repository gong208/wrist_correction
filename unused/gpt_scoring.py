import os
import glob
import base64
import json
from typing import List, Dict, Tuple, Optional
from PIL import Image
from io import BytesIO
from openai import OpenAI

def _encode_image_to_data_url(path: str) -> str:
    # Encode image as data URL (base64) so it can be sent inline to the API
    with Image.open(path) as im:
        # (Optional) downscale very large images to reduce token/cost/latency
        # im.thumbnail((1024, 1024), Image.LANCZOS)
        buf = BytesIO()
        im.save(buf, format="PNG")
        b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    return f"data:image/png;base64,{b64}"

def _build_vision_message(instruction: str, frame_idx: int, angle_paths: Dict[str, str]):
    """
    Build a single multimodal message: instruction text + 4 angle images for this frame.
    """
    # Ask for a structured JSON answer to make scoring easy to parse later
    system_prompt = (
        "You are a strict motion judge. Score how well the motion matches the instruction.\n"
        "Return STRICT JSON with keys: score (0-5 integer) and reason (string under 50 words)."
    )
    user_text = (
        f"Instruction: {instruction}\n"
        f"Below are four views (front/back/left/right) for frame {frame_idx:04d}. "
        "Score the quality of the motion generated by the model based on the following rubrics:\n"
        "1: Human body is in a bad posture.\n"
        "2: Human body is interacting with the object in an unatural way.\n"
        "3: Human body is penetration into the object in most of the frames.\n"
        "4: Human body is penetrating into the object in some frames.\n"
        "5: Human is interacting with the object naturally and smoothly.\n"
        "Return JSON only."
    )

    content = [{"type": "text", "text": user_text}]
    # Add images in a consistent order
    for ang in ("front", "back", "left", "right"):
        p = angle_paths.get(ang)
        if p and os.path.exists(p):
            content.append({
                "type": "image_url",
                "image_url": {"url": _encode_image_to_data_url(p)}
            })
    return system_prompt, content

def _safe_parse_json(s: str) -> Optional[Dict]:
    try:
        return json.loads(s)
    except Exception:
        # Try to extract a JSON object if the model added extra text
        start = s.find("{")
        end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(s[start:end+1])
            except Exception:
                return None
        return None

def score_motion_frames_with_gpt(
    save_path: str,
    instruction: str,
    frame_stride: int = 8,
    model: str = "gpt-4o",          # vision-capable model
    temperature: float = 0.0,
) -> List[Dict[str, object]]:
    """
    Sample every `frame_stride` frames from `save_path`. For each sampled frame, read
    all 4 angles (front/back/left/right), send as a single GPT-4o request with the
    instruction, and collect a JSON score + reason.

    Returns: list of dicts like:
        {
          "frame_idx": 0,
          "score": 7,
          "reason": "…",
          "raw_response": "<model text>"
        }
    """
    client = OpenAI()

    # Find available frame indices from front view presence
    front_glob = glob.glob(os.path.join(save_path, "frame_*_front.png"))
    # Expect filenames like frame_0000_front.png
    def _extract_idx(p):
        base = os.path.basename(p)  # frame_0000_front.png
        # split -> ["frame","0000","front.png"] or ["frame","0000","front","png"] depending on naming
        parts = base.split("_")
        # robust: second part should be the index, remove extension from last part elsewhere
        return int(parts[1])

    all_front = sorted(front_glob, key=lambda p: _extract_idx(p))
    all_indices = [_extract_idx(p) for p in all_front]
    sampled = all_indices[::max(1, frame_stride)]

    results = []
    for idx in sampled:
        # Build the four angle paths for this frame
        angle_paths = {
            "front": os.path.join(save_path, f"frame_{idx:04d}_front.png"),
            "back":  os.path.join(save_path, f"frame_{idx:04d}_back.png"),
            "left":  os.path.join(save_path, f"frame_{idx:04d}_left.png"),
            "right": os.path.join(save_path, f"frame_{idx:04d}_right.png"),
        }

        # Require at least front view; optionally skip if any angle missing:
        if not os.path.exists(angle_paths["front"]):
            continue  # skip frames without the primary view
        # If you want to enforce all-angles, uncomment:
        # if not all(os.path.exists(p) for p in angle_paths.values()):
        #     continue

        system_prompt, content = _build_vision_message(instruction, idx, angle_paths)

        resp = client.chat.completions.create(
            model=model,
            temperature=temperature,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content},
            ],
        )

        text = resp.choices[0].message.content.strip()
        parsed = _safe_parse_json(text) or {}

        # Normalize output
        score = parsed.get("score", None)
        reason = parsed.get("reason", "").strip() if isinstance(parsed.get("reason", ""), str) else ""

        # Coerce score to int in [0,10] if possible
        if isinstance(score, (int, float)):
            try:
                score = int(round(float(score)))
                score = max(0, min(10, score))
            except Exception:
                pass

        results.append({
            "frame_idx": idx,
            "score": score,
            "reason": reason,
            "raw_response": text,
        })

    return results


def score_motion_all_sampled_frames_once(
    save_path: str,
    instruction: str,
    frame_stride: int = 8,
    angles: Tuple[str, ...] = ("front", "back", "left", "right"),
    model: str = "gpt-4o",
    temperature: float = 0.0,
    require_all_angles: bool = True,
) -> Dict:
    """
    Build ONE multimodal request that includes every sampled frame (every `frame_stride`)
    and, for each frame, all specified `angles`. Asks GPT to return JSON with:
      {
        "overall_score": int (0-10),
        "overall_reason": "…",
        "frames": [
          {"frame_idx": 0, "score": int (0-10), "reason": "…"},
          ...
        ]
      }

    Returns the parsed JSON (plus raw text in ["_raw"]) from the model.
    """

    # Discover frames from 'front' files (assumed canonical)
    front_paths = sorted(glob.glob(os.path.join(save_path, "frame_*_front.png")))
    if not front_paths:
        raise FileNotFoundError(f"No frames like frame_*_front.png found under {save_path}")

    def _idx(p: str) -> int:
        # expects frame_0000_front.png
        base = os.path.basename(p)
        return int(base.split("_")[1])

    indices = [_idx(p) for p in front_paths]
    sampled = indices[::max(1, frame_stride)]

    # Optionally enforce that all angle files exist for each sampled frame
    if require_all_angles:
        def _all_exist(i: int) -> bool:
            return all(os.path.exists(os.path.join(save_path, f"frame_{i:04d}_{a}.png")) for a in angles)
        sampled = [i for i in sampled if _all_exist(i)]
        if not sampled:
            raise FileNotFoundError("No sampled frames have all requested angle images present.")

    # Build ONE content payload: interleave a small text label per frame and its four images
    system_prompt = (
        "You are a strict motion judge. Evaluate how well the motion matches the instruction. "
        "Return STRICT JSON with keys: overall_score (0-5 integer), overall_reason (<=50 words), "
        "No extra text outside JSON."
    )

    header_text = (
        "Instruction: " + instruction + "\n"
        "You will see multiple frames from 4 views each (order: front, back, left, right). "
        "Score the quality of the motion generated by the model based on the following rubrics:\n"
        "1: Human body is in a bad posture.\n"
        "2: Human body is interacting with the object in an unatural way.\n"
        "3: Human body is penetration into the object in most of the frames.\n"
        "4: Human body is penetrating into the object in some frames.\n"
        "5: Human is interacting with the object naturally and smoothly.\n"
        "Score the overall quality of the motion after viewing all the frames. "
        "Return JSON only."
    )

    content = [{"type": "text", "text": header_text}]

    # NOTE: This is still a single API call; we just build the content list here.
    # (Using a comprehension is fine; requirement is 'not per-frame API calls'.)
    for i in sampled:
        content.append({"type": "text", "text": f"Frame {i:04d} (views in order: {', '.join(angles)})"})
        for a in angles:
            img_path = os.path.join(save_path, f"frame_{i:04d}_{a}.png")
            if not os.path.exists(img_path):
                # If strict is desired, you could raise; otherwise skip missing angle.
                continue
            content.append({
                "type": "image_url",
                "image_url": {"url": _encode_image_to_data_url(img_path)}
            })

    client = OpenAI()
    resp = client.chat.completions.create(
        model=model,
        temperature=temperature,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": content},
        ],
    )

    raw = resp.choices[0].message.content.strip()
    parsed = _safe_parse_json(raw) or {}
    parsed["_raw"] = raw

    # Light normalization on per-frame scores
    # if "frames" in parsed and isinstance(parsed["frames"], list):
    #     for f in parsed["frames"]:
    #         if isinstance(f.get("score", None), (int, float)):
    #             v = max(0, min(10, int(round(float(f["score"])))))
    #             f["score"] = v
    if "overall_score" in parsed and isinstance(parsed["overall_score"], (int, float)):
        parsed["overall_score"] = max(0, min(5, int(round(float(parsed["overall_score"])))))

    return parsed

if __name__ == "__main__":
    save_path = "visualization_output/sub3_monitor_005"
    instruction = "The human is interacting with the object naturally and smoothly."
    results = score_motion_frames_with_gpt(save_path, instruction)
    print(results)